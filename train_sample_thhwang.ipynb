{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1 True\n",
      "0.28.0\n"
     ]
    }
   ],
   "source": [
    "import mmcv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import ade_custom\n",
    "import dataset_split as ds\n",
    "import matplotlib.patches as mpatches\n",
    "import random\n",
    "\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "import mmseg\n",
    "print(mmseg.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Add a new dataset\n",
    "data_root, img_dir, ann_dir 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shutil\n",
    "import random\n",
    "from glob import glob\n",
    "import json\n",
    "\n",
    "# data 경로 설정\n",
    "data_root = '/data/36-3/'\n",
    "img_path = data_root + 'img_dir/'\n",
    "ann_path = data_root + 'ann_dir/'\n",
    "img_paths = glob(img_path+'*')\n",
    "ann_paths = glob(ann_path+'*')\n",
    "img_move_path = data_root + 'img_val/'\n",
    "ann_move_path = data_root + 'ann_val/'\n",
    "img_move_path_t = data_root + 'img_test/'\n",
    "ann_move_path_t = data_root + 'ann_test/'\n",
    "label_path = '/data/36-3/label_data/'\n",
    "label_folder = glob(label_path+'*')\n",
    "ds.root = data_root\n",
    "ds.label_path = label_path\n",
    "# new dataset classes, class별 색상\n",
    "classes = ade_custom.COLOR_PARAM.CLASSES\n",
    "palette = ade_custom.COLOR_PARAM.COLORMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Check Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Let's take a look at the segmentation map we got\n",
    "# img = Image.open('/mmsegmentation/data/36-3/ann_dir/24_193937_220616_170.png')\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# im = plt.imshow(np.array(img.convert('RGB')))\n",
    "\n",
    "# # create a patch (proxy artist) for every color \n",
    "# patches = [mpatches.Patch(color=np.array(palette[i])/255., \n",
    "#                           label=classes[i]) for i in range(8)]\n",
    "# # put those patched as legend-handles into the legend\n",
    "# plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., \n",
    "#            fontsize='large')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split dataset randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restore Done!\n"
     ]
    }
   ],
   "source": [
    "# split 한 데이터 다시 원래 이미지경로로 복원\n",
    "ds.restore_split(img_path, ann_path,img_move_path, ann_move_path, img_move_path_t, ann_move_path_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, file_list = ds.make_label_list(label_folder)\n",
    "classes = ds.dict_key_lower(classes)\n",
    "file_list = ds.dict_key_lower(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_label_split finish!\n",
      "label_split finish!\n"
     ]
    }
   ],
   "source": [
    "ds.s_label_split(img_path, ann_path, img_move_path, ann_move_path, img_move_path_t, ann_move_path_t, classes, file_list)\n",
    "ds.label_split(img_path, ann_path, img_move_path, ann_move_path, img_move_path_t, ann_move_path_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7461\n",
      "912\n",
      "907\n",
      "7261\n",
      "912\n",
      "907\n"
     ]
    }
   ],
   "source": [
    "print(len(glob('/data/36-3/img_dir/*')))\n",
    "print(len(glob('/data/36-3/img_val/*')))\n",
    "print(len(glob('/data/36-3/img_test/*')))\n",
    "\n",
    "print(len(glob('/data/36-3/ann_dir/*')))\n",
    "print(len(glob('/data/36-3/ann_val/*')))\n",
    "print(len(glob('/data/36-3/ann_test/*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train:valid:test / 8:1:1 비율로 split\n",
    "split_dir = 'splits'\n",
    "mmcv.mkdir_or_exist(osp.join(data_root, split_dir))\n",
    "origin_list = [osp.splitext(filename)[0] for filename in mmcv.scandir(\n",
    "    osp.join(data_root, ann_path), suffix='.png')]\n",
    "valid_list = [osp.splitext(filename)[0] for filename in mmcv.scandir(\n",
    "    osp.join(data_root, ann_move_path), suffix='.png')]\n",
    "test_list = [osp.splitext(filename)[0] for filename in mmcv.scandir(\n",
    "    osp.join(data_root, ann_move_path_t), suffix='.png')]\n",
    "\n",
    "random.shuffle(origin_list)\n",
    "with open(osp.join(data_root, split_dir, 'train.txt'), 'w') as f:\n",
    "  f.writelines(line + '\\n' for line in origin_list)\n",
    "\n",
    "random.shuffle(valid_list)\n",
    "with open(osp.join(data_root, split_dir, 'val.txt'), 'w') as f:\n",
    "  f.writelines(line + '\\n' for line in valid_list)\n",
    "\n",
    "random.shuffle(test_list)\n",
    "with open(osp.join(data_root, split_dir, 'test.txt'), 'w') as f:\n",
    "  f.writelines(line + '\\n' for line in test_list)\n",
    "\n",
    "# with open(osp.join(data_root, split_dir, 'val.txt'), 'w') as f:\n",
    "#   val_length = int(len(filename_list)*9/10)\n",
    "#   f.writelines(line + '\\n' for line in filename_list[train_length:val_length])\n",
    "\n",
    "# with open(osp.join(data_root, split_dir, 'test.txt'), 'w') as f:\n",
    "#   f.writelines(line + '\\n' for line in filename_list[val_length:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmseg.datasets.builder import DATASETS\n",
    "from mmseg.datasets.custom import CustomDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "# cfg = Config.fromfile('/home/mmsegmentation/configs/swin/upernet_swin_base_patch4_window12_512x512_160k_ade20k_pretrain_384x384_22K.py')\n",
    "\n",
    "# cfg = Config.fromfile('/home/mmsegmentation/custom_config_train.py')\n",
    "\n",
    "cfg = Config.fromfile('/home/mmsegmentation/custom_config_train_3.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "norm_cfg = dict(type='BN', requires_grad=True)\n",
      "backbone_norm_cfg = dict(type='LN', requires_grad=True)\n",
      "model = dict(\n",
      "    type='EncoderDecoder',\n",
      "    pretrained=None,\n",
      "    backbone=dict(\n",
      "        type='SwinTransformer',\n",
      "        pretrain_img_size=384,\n",
      "        embed_dims=128,\n",
      "        patch_size=4,\n",
      "        window_size=12,\n",
      "        mlp_ratio=4,\n",
      "        depths=[2, 2, 18, 2],\n",
      "        num_heads=[4, 8, 16, 32],\n",
      "        strides=(4, 2, 2, 2),\n",
      "        out_indices=(0, 1, 2, 3),\n",
      "        qkv_bias=True,\n",
      "        qk_scale=None,\n",
      "        patch_norm=True,\n",
      "        drop_rate=0.0,\n",
      "        attn_drop_rate=0.0,\n",
      "        drop_path_rate=0.3,\n",
      "        use_abs_pos_embed=False,\n",
      "        act_cfg=dict(type='GELU'),\n",
      "        norm_cfg=dict(type='LN', requires_grad=True),\n",
      "        init_cfg=dict(\n",
      "            type='Pretrained',\n",
      "            checkpoint=\n",
      "            '/home/mmsegmentation/checkpoints/swin_base_patch4_window12_384_22k.pth'\n",
      "        )),\n",
      "    decode_head=dict(\n",
      "        type='UPerHead',\n",
      "        in_channels=[128, 256, 512, 1024],\n",
      "        in_index=[0, 1, 2, 3],\n",
      "        pool_scales=(1, 2, 3, 6),\n",
      "        channels=512,\n",
      "        dropout_ratio=0.1,\n",
      "        num_classes=32,\n",
      "        norm_cfg=dict(type='BN', requires_grad=True),\n",
      "        align_corners=False,\n",
      "        loss_decode=dict(\n",
      "            type='CrossEntropyLoss',\n",
      "            use_sigmoid=False,\n",
      "            loss_weight=1.0,\n",
      "            avg_non_ignore=True,\n",
      "            class_weight=[\n",
      "                0.1728, 0.2964, 6.3734, 1.0727, 134.3023, 5573.5469, 185.7849,\n",
      "                6.4064, 655.7114, 857.4688, 11147.0938, 28.7296, 655.7114,\n",
      "                0.5146, 0.5167, 58.6689, 15.125, 1.0736, 11147.0938, 0.4181,\n",
      "                0.1446, 0.7899, 0.5379, 2786.7734, 557.3547, 359.5837,\n",
      "                11147.0938, 14.0392, 0.5675, 11.1583, 11147.0938, 0.437\n",
      "            ])),\n",
      "    auxiliary_head=dict(\n",
      "        type='FCNHead',\n",
      "        in_channels=512,\n",
      "        in_index=2,\n",
      "        channels=256,\n",
      "        num_convs=1,\n",
      "        concat_input=False,\n",
      "        dropout_ratio=0.1,\n",
      "        num_classes=32,\n",
      "        norm_cfg=dict(type='BN', requires_grad=True),\n",
      "        align_corners=False,\n",
      "        loss_decode=dict(\n",
      "            type='CrossEntropyLoss',\n",
      "            use_sigmoid=False,\n",
      "            loss_weight=0.4,\n",
      "            avg_non_ignore=True,\n",
      "            class_weight=[\n",
      "                0.1728, 0.2964, 6.3734, 1.0727, 134.3023, 5573.5469, 185.7849,\n",
      "                6.4064, 655.7114, 857.4688, 11147.0938, 28.7296, 655.7114,\n",
      "                0.5146, 0.5167, 58.6689, 15.125, 1.0736, 11147.0938, 0.4181,\n",
      "                0.1446, 0.7899, 0.5379, 2786.7734, 557.3547, 359.5837,\n",
      "                11147.0938, 14.0392, 0.5675, 11.1583, 11147.0938, 0.437\n",
      "            ])),\n",
      "    train_cfg=dict(),\n",
      "    test_cfg=dict(mode='whole'))\n",
      "dataset_type = 'CustomDataset'\n",
      "data_root = '/data/36-3/'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[106.93, 108.47, 112.98], std=[43.48, 44.15, 50.01], to_rgb=True)\n",
      "crop_size = (512, 512)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', reduce_zero_label=False),\n",
      "    dict(type='Resize', img_scale=(512, 512)),\n",
      "    dict(type='RandomFlip', flip_ratio=0.5),\n",
      "    dict(type='PhotoMetricDistortion'),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[106.93, 108.47, 112.98],\n",
      "        std=[43.48, 44.15, 50.01],\n",
      "        to_rgb=True),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(type='Collect', keys=['img', 'gt_semantic_seg'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(512, 512),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[106.93, 108.47, 112.98],\n",
      "                std=[43.48, 44.15, 50.01],\n",
      "                to_rgb=True),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data = dict(\n",
      "    samples_per_gpu=16,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='CustomDataset',\n",
      "        data_root='/data/36-3/',\n",
      "        img_dir='/data/36-3/img_dir/',\n",
      "        ann_dir='/data/36-3/ann_dir/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', reduce_zero_label=False),\n",
      "            dict(type='Resize', img_scale=(512, 512)),\n",
      "            dict(type='RandomFlip', flip_ratio=0.5),\n",
      "            dict(type='PhotoMetricDistortion'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[106.93, 108.47, 112.98],\n",
      "                std=[43.48, 44.15, 50.01],\n",
      "                to_rgb=True),\n",
      "            dict(type='DefaultFormatBundle'),\n",
      "            dict(type='Collect', keys=['img', 'gt_semantic_seg'])\n",
      "        ],\n",
      "        split='/data/36-3/splits/train.txt'),\n",
      "    val=dict(\n",
      "        type='CustomDataset',\n",
      "        data_root='/data/36-3/',\n",
      "        img_dir='/data/36-3/img_val/',\n",
      "        ann_dir='/data/36-3/ann_val/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(512, 512),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[106.93, 108.47, 112.98],\n",
      "                        std=[43.48, 44.15, 50.01],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        split='/data/36-3/splits/val.txt'),\n",
      "    test=dict(\n",
      "        type='CustomDataset',\n",
      "        data_root='/data/36-3/',\n",
      "        img_dir='/data/36-3/img_test/',\n",
      "        ann_dir='/data/36-3/ann_test/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(512, 512),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[106.93, 108.47, 112.98],\n",
      "                        std=[43.48, 44.15, 50.01],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        split='/data/36-3/splits/test.txt'))\n",
      "log_config = dict(\n",
      "    interval=200, hooks=[dict(type='TextLoggerHook', by_epoch=False)])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = '/home/mmsegmentation/checkpoints/swin_base_patch4_window12_384_22k.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "cudnn_benchmark = True\n",
      "optimizer = dict(\n",
      "    type='AdamW',\n",
      "    lr=6e-05,\n",
      "    betas=(0.9, 0.999),\n",
      "    weight_decay=0.01,\n",
      "    paramwise_cfg=dict(\n",
      "        custom_keys=dict(\n",
      "            absolute_pos_embed=dict(decay_mult=0.0),\n",
      "            relative_position_bias_table=dict(decay_mult=0.0),\n",
      "            norm=dict(decay_mult=0.0))))\n",
      "optimizer_config = dict()\n",
      "lr_config = dict(\n",
      "    policy='poly',\n",
      "    warmup='linear',\n",
      "    warmup_iters=1500,\n",
      "    warmup_ratio=1e-06,\n",
      "    power=1.0,\n",
      "    min_lr=0.0,\n",
      "    by_epoch=False)\n",
      "runner = dict(type='IterBasedRunner', max_iters=160000)\n",
      "checkpoint_config = dict(by_epoch=False, interval=16000)\n",
      "evaluation = dict(interval=800, metric='mIoU', pre_eval=True)\n",
      "checkpoint_file = '/home/mmsegmentation/checkpoints/swin_base_patch4_window12_384_22k.pth'\n",
      "work_dir = '/data/result/36-3/test1006/'\n",
      "seed = 42\n",
      "gpu_ids = range(0, 1)\n",
      "device = 'cuda'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmseg.apis import set_random_seed\n",
    "from mmseg.utils import get_device\n",
    "\n",
    "# Since we use only one GPU, BN is used instead of SyncBN\n",
    "cfg.norm_cfg = dict(type='BN', requires_grad=True)\n",
    "cfg.model.backbone.norm_cfg = dict(type='LN', requires_grad=True)\n",
    "cfg.model.decode_head.norm_cfg = cfg.norm_cfg\n",
    "cfg.model.auxiliary_head.norm_cfg = cfg.norm_cfg\n",
    "# modify num classes of the model in decode/auxiliary head\n",
    "cfg.model.decode_head.num_classes = 32\n",
    "cfg.model.auxiliary_head.num_classes = 32\n",
    "\n",
    "# Modify dataset type and path\n",
    "cfg.dataset_type = 'CustomDataset'\n",
    "cfg.data_root = '/data/36-3/'\n",
    "# batch_size\n",
    "cfg.data.samples_per_gpu = 16\n",
    "cfg.data.workers_per_gpu= 4\n",
    "\n",
    "cfg.img_norm_cfg = dict(\n",
    "    # mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
    "      mean=[106.93, 108.47, 112.98],std=[43.48, 44.15, 50.01], to_rgb=True)\n",
    "    # 36-4 mean,std\n",
    "    # mean=[41.62, 37.78, 34.45],std=[21.30, 17.74, 16.80], to_rgb=True)\n",
    "    #mean=[0, 0, 0], std=[1, 1, 1], to_rgb=True)\n",
    "    # mean=[106.84, 108.37, 112.71], std=[43.01, 43.76, 49.51], to_rgb=True)\n",
    "#cfg.crop_size = (256, 256)\n",
    "cfg.train_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='LoadAnnotations', reduce_zero_label = False),\n",
    "    dict(type='Resize', img_scale=(512, 512)),\n",
    "    #dict(type='RandomCrop', crop_size=cfg.crop_size, cat_max_ratio=0.75),\n",
    "    dict(type='RandomFlip', flip_ratio=0.5),\n",
    "    dict(type='PhotoMetricDistortion'),\n",
    "    dict(type='Normalize', **cfg.img_norm_cfg),\n",
    "    #dict(type='Pad', size=cfg.crop_size, pad_val=0, seg_pad_val=255),\n",
    "    dict(type='DefaultFormatBundle'),\n",
    "    dict(type='Collect', keys=['img', 'gt_semantic_seg']),\n",
    "]\n",
    " \n",
    "cfg.test_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(\n",
    "        type='MultiScaleFlipAug',\n",
    "        img_scale=(512, 512),\n",
    "        # img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\n",
    "        flip=False,\n",
    "        transforms=[\n",
    "            dict(type='Resize', keep_ratio=True),\n",
    "            dict(type='RandomFlip'),\n",
    "            dict(type='Normalize', **cfg.img_norm_cfg),\n",
    "            dict(type='ImageToTensor', keys=['img']),\n",
    "            dict(type='Collect', keys=['img']),\n",
    "        ])\n",
    "]\n",
    "\n",
    "\n",
    "cfg.data.train.type = cfg.dataset_type\n",
    "cfg.data.train.data_root = cfg.data_root\n",
    "cfg.data.train.img_dir = img_path\n",
    "cfg.data.train.ann_dir = ann_path\n",
    "cfg.data.train.pipeline = cfg.train_pipeline\n",
    "cfg.data.train.split = data_root+'splits/train.txt'\n",
    "\n",
    "cfg.data.val.type = cfg.dataset_type\n",
    "cfg.data.val.data_root = cfg.data_root\n",
    "cfg.data.val.img_dir = img_move_path\n",
    "cfg.data.val.ann_dir = ann_move_path\n",
    "cfg.data.val.pipeline = cfg.test_pipeline\n",
    "cfg.data.val.split = data_root+'splits/val.txt'\n",
    "\n",
    "cfg.data.test.type = cfg.dataset_type\n",
    "cfg.data.test.data_root = cfg.data_root\n",
    "cfg.data.test.img_dir = img_move_path_t\n",
    "cfg.data.test.ann_dir = ann_move_path_t\n",
    "cfg.data.test.pipeline = cfg.test_pipeline\n",
    "cfg.data.test.split = data_root+'splits/test.txt'\n",
    "\n",
    "# We can still use the pre-trained Mask RCNN model though we do not need to\n",
    "# use the mask branch\n",
    "cfg.load_from = '/home/mmsegmentation/checkpoints/swin_base_patch4_window12_384_22k.pth'\n",
    "\n",
    "# cfg.load_from = 'data/result/36-3/test0913/iter_1600.pth'\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = '/data/result/36-3/test1006/'\n",
    "\n",
    "cfg.runner.max_iters = 160000\n",
    "cfg.log_config.interval = 200\n",
    "cfg.evaluation.interval = 800\n",
    "cfg.checkpoint_config.interval = 16000\n",
    "\n",
    "# Set seed to facitate reproducing the result\n",
    "cfg.seed = 42\n",
    "set_random_seed(42, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "cfg.device = get_device()\n",
    "\n",
    "# Let's have a look at the final config used for training\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mmseg.apis import set_random_seed\n",
    "# from mmseg.utils import get_device\n",
    "\n",
    "# # Modify dataset type and path\n",
    "# cfg.dataset_type = 'ADE20KDataset'\n",
    "# cfg.data_root = '/mmsegmentation/data/36-3/'\n",
    "\n",
    "# # We can still use the pre-trained Mask RCNN model though we do not need to\n",
    "# # use the mask branch\n",
    "# # cfg.load_from = '/mmsegmentation/upernet_swin_base_patch4_window12_512x512_160k_ade20k_pretrain_384x384_22K_20210531_125459-429057bf.pth'\n",
    "\n",
    "# cfg.load_from = '/mmsegmentation/data/result/36-3/test0923/latest.pth'\n",
    "# # Set up working dir to save files and logs.\n",
    "# cfg.work_dir = '/mmsegmentation/data/result/36-3/test0927/'\n",
    "\n",
    "\n",
    "# # Set seed to facitate reproducing the result\n",
    "# cfg.seed = 42\n",
    "# set_random_seed(42, deterministic=False)\n",
    "# cfg.gpu_ids = range(1)\n",
    "# cfg.device = get_device()\n",
    "\n",
    "# # Let's have a look at the final config used for training\n",
    "# print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-06 09:56:45,578 - mmseg - INFO - Loaded 7261 images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-06 09:56:48,201 - mmseg - INFO - Loaded 912 images\n",
      "2022-10-06 09:56:48,213 - mmseg - INFO - load checkpoint from local path: /home/mmsegmentation/checkpoints/swin_base_patch4_window12_384_22k.pth\n",
      "2022-10-06 09:56:48,472 - mmseg - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: model\n",
      "\n",
      "missing keys in source state_dict: backbone.patch_embed.projection.weight, backbone.patch_embed.projection.bias, backbone.patch_embed.norm.weight, backbone.patch_embed.norm.bias, backbone.stages.0.blocks.0.norm1.weight, backbone.stages.0.blocks.0.norm1.bias, backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table, backbone.stages.0.blocks.0.attn.w_msa.relative_position_index, backbone.stages.0.blocks.0.attn.w_msa.qkv.weight, backbone.stages.0.blocks.0.attn.w_msa.qkv.bias, backbone.stages.0.blocks.0.attn.w_msa.proj.weight, backbone.stages.0.blocks.0.attn.w_msa.proj.bias, backbone.stages.0.blocks.0.norm2.weight, backbone.stages.0.blocks.0.norm2.bias, backbone.stages.0.blocks.0.ffn.layers.0.0.weight, backbone.stages.0.blocks.0.ffn.layers.0.0.bias, backbone.stages.0.blocks.0.ffn.layers.1.weight, backbone.stages.0.blocks.0.ffn.layers.1.bias, backbone.stages.0.blocks.1.norm1.weight, backbone.stages.0.blocks.1.norm1.bias, backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table, backbone.stages.0.blocks.1.attn.w_msa.relative_position_index, backbone.stages.0.blocks.1.attn.w_msa.qkv.weight, backbone.stages.0.blocks.1.attn.w_msa.qkv.bias, backbone.stages.0.blocks.1.attn.w_msa.proj.weight, backbone.stages.0.blocks.1.attn.w_msa.proj.bias, backbone.stages.0.blocks.1.norm2.weight, backbone.stages.0.blocks.1.norm2.bias, backbone.stages.0.blocks.1.ffn.layers.0.0.weight, backbone.stages.0.blocks.1.ffn.layers.0.0.bias, backbone.stages.0.blocks.1.ffn.layers.1.weight, backbone.stages.0.blocks.1.ffn.layers.1.bias, backbone.stages.0.downsample.norm.weight, backbone.stages.0.downsample.norm.bias, backbone.stages.0.downsample.reduction.weight, backbone.stages.1.blocks.0.norm1.weight, backbone.stages.1.blocks.0.norm1.bias, backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table, backbone.stages.1.blocks.0.attn.w_msa.relative_position_index, backbone.stages.1.blocks.0.attn.w_msa.qkv.weight, backbone.stages.1.blocks.0.attn.w_msa.qkv.bias, backbone.stages.1.blocks.0.attn.w_msa.proj.weight, backbone.stages.1.blocks.0.attn.w_msa.proj.bias, backbone.stages.1.blocks.0.norm2.weight, backbone.stages.1.blocks.0.norm2.bias, backbone.stages.1.blocks.0.ffn.layers.0.0.weight, backbone.stages.1.blocks.0.ffn.layers.0.0.bias, backbone.stages.1.blocks.0.ffn.layers.1.weight, backbone.stages.1.blocks.0.ffn.layers.1.bias, backbone.stages.1.blocks.1.norm1.weight, backbone.stages.1.blocks.1.norm1.bias, backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table, backbone.stages.1.blocks.1.attn.w_msa.relative_position_index, backbone.stages.1.blocks.1.attn.w_msa.qkv.weight, backbone.stages.1.blocks.1.attn.w_msa.qkv.bias, backbone.stages.1.blocks.1.attn.w_msa.proj.weight, backbone.stages.1.blocks.1.attn.w_msa.proj.bias, backbone.stages.1.blocks.1.norm2.weight, backbone.stages.1.blocks.1.norm2.bias, backbone.stages.1.blocks.1.ffn.layers.0.0.weight, backbone.stages.1.blocks.1.ffn.layers.0.0.bias, backbone.stages.1.blocks.1.ffn.layers.1.weight, backbone.stages.1.blocks.1.ffn.layers.1.bias, backbone.stages.1.downsample.norm.weight, backbone.stages.1.downsample.norm.bias, backbone.stages.1.downsample.reduction.weight, backbone.stages.2.blocks.0.norm1.weight, backbone.stages.2.blocks.0.norm1.bias, backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.0.attn.w_msa.relative_position_index, backbone.stages.2.blocks.0.attn.w_msa.qkv.weight, backbone.stages.2.blocks.0.attn.w_msa.qkv.bias, backbone.stages.2.blocks.0.attn.w_msa.proj.weight, backbone.stages.2.blocks.0.attn.w_msa.proj.bias, backbone.stages.2.blocks.0.norm2.weight, backbone.stages.2.blocks.0.norm2.bias, backbone.stages.2.blocks.0.ffn.layers.0.0.weight, backbone.stages.2.blocks.0.ffn.layers.0.0.bias, backbone.stages.2.blocks.0.ffn.layers.1.weight, backbone.stages.2.blocks.0.ffn.layers.1.bias, backbone.stages.2.blocks.1.norm1.weight, backbone.stages.2.blocks.1.norm1.bias, backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.1.attn.w_msa.relative_position_index, backbone.stages.2.blocks.1.attn.w_msa.qkv.weight, backbone.stages.2.blocks.1.attn.w_msa.qkv.bias, backbone.stages.2.blocks.1.attn.w_msa.proj.weight, backbone.stages.2.blocks.1.attn.w_msa.proj.bias, backbone.stages.2.blocks.1.norm2.weight, backbone.stages.2.blocks.1.norm2.bias, backbone.stages.2.blocks.1.ffn.layers.0.0.weight, backbone.stages.2.blocks.1.ffn.layers.0.0.bias, backbone.stages.2.blocks.1.ffn.layers.1.weight, backbone.stages.2.blocks.1.ffn.layers.1.bias, backbone.stages.2.blocks.2.norm1.weight, backbone.stages.2.blocks.2.norm1.bias, backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.2.attn.w_msa.relative_position_index, backbone.stages.2.blocks.2.attn.w_msa.qkv.weight, backbone.stages.2.blocks.2.attn.w_msa.qkv.bias, backbone.stages.2.blocks.2.attn.w_msa.proj.weight, backbone.stages.2.blocks.2.attn.w_msa.proj.bias, backbone.stages.2.blocks.2.norm2.weight, backbone.stages.2.blocks.2.norm2.bias, backbone.stages.2.blocks.2.ffn.layers.0.0.weight, backbone.stages.2.blocks.2.ffn.layers.0.0.bias, backbone.stages.2.blocks.2.ffn.layers.1.weight, backbone.stages.2.blocks.2.ffn.layers.1.bias, backbone.stages.2.blocks.3.norm1.weight, backbone.stages.2.blocks.3.norm1.bias, backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.3.attn.w_msa.relative_position_index, backbone.stages.2.blocks.3.attn.w_msa.qkv.weight, backbone.stages.2.blocks.3.attn.w_msa.qkv.bias, backbone.stages.2.blocks.3.attn.w_msa.proj.weight, backbone.stages.2.blocks.3.attn.w_msa.proj.bias, backbone.stages.2.blocks.3.norm2.weight, backbone.stages.2.blocks.3.norm2.bias, backbone.stages.2.blocks.3.ffn.layers.0.0.weight, backbone.stages.2.blocks.3.ffn.layers.0.0.bias, backbone.stages.2.blocks.3.ffn.layers.1.weight, backbone.stages.2.blocks.3.ffn.layers.1.bias, backbone.stages.2.blocks.4.norm1.weight, backbone.stages.2.blocks.4.norm1.bias, backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.4.attn.w_msa.relative_position_index, backbone.stages.2.blocks.4.attn.w_msa.qkv.weight, backbone.stages.2.blocks.4.attn.w_msa.qkv.bias, backbone.stages.2.blocks.4.attn.w_msa.proj.weight, backbone.stages.2.blocks.4.attn.w_msa.proj.bias, backbone.stages.2.blocks.4.norm2.weight, backbone.stages.2.blocks.4.norm2.bias, backbone.stages.2.blocks.4.ffn.layers.0.0.weight, backbone.stages.2.blocks.4.ffn.layers.0.0.bias, backbone.stages.2.blocks.4.ffn.layers.1.weight, backbone.stages.2.blocks.4.ffn.layers.1.bias, backbone.stages.2.blocks.5.norm1.weight, backbone.stages.2.blocks.5.norm1.bias, backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.5.attn.w_msa.relative_position_index, backbone.stages.2.blocks.5.attn.w_msa.qkv.weight, backbone.stages.2.blocks.5.attn.w_msa.qkv.bias, backbone.stages.2.blocks.5.attn.w_msa.proj.weight, backbone.stages.2.blocks.5.attn.w_msa.proj.bias, backbone.stages.2.blocks.5.norm2.weight, backbone.stages.2.blocks.5.norm2.bias, backbone.stages.2.blocks.5.ffn.layers.0.0.weight, backbone.stages.2.blocks.5.ffn.layers.0.0.bias, backbone.stages.2.blocks.5.ffn.layers.1.weight, backbone.stages.2.blocks.5.ffn.layers.1.bias, backbone.stages.2.blocks.6.norm1.weight, backbone.stages.2.blocks.6.norm1.bias, backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.6.attn.w_msa.relative_position_index, backbone.stages.2.blocks.6.attn.w_msa.qkv.weight, backbone.stages.2.blocks.6.attn.w_msa.qkv.bias, backbone.stages.2.blocks.6.attn.w_msa.proj.weight, backbone.stages.2.blocks.6.attn.w_msa.proj.bias, backbone.stages.2.blocks.6.norm2.weight, backbone.stages.2.blocks.6.norm2.bias, backbone.stages.2.blocks.6.ffn.layers.0.0.weight, backbone.stages.2.blocks.6.ffn.layers.0.0.bias, backbone.stages.2.blocks.6.ffn.layers.1.weight, backbone.stages.2.blocks.6.ffn.layers.1.bias, backbone.stages.2.blocks.7.norm1.weight, backbone.stages.2.blocks.7.norm1.bias, backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.7.attn.w_msa.relative_position_index, backbone.stages.2.blocks.7.attn.w_msa.qkv.weight, backbone.stages.2.blocks.7.attn.w_msa.qkv.bias, backbone.stages.2.blocks.7.attn.w_msa.proj.weight, backbone.stages.2.blocks.7.attn.w_msa.proj.bias, backbone.stages.2.blocks.7.norm2.weight, backbone.stages.2.blocks.7.norm2.bias, backbone.stages.2.blocks.7.ffn.layers.0.0.weight, backbone.stages.2.blocks.7.ffn.layers.0.0.bias, backbone.stages.2.blocks.7.ffn.layers.1.weight, backbone.stages.2.blocks.7.ffn.layers.1.bias, backbone.stages.2.blocks.8.norm1.weight, backbone.stages.2.blocks.8.norm1.bias, backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.8.attn.w_msa.relative_position_index, backbone.stages.2.blocks.8.attn.w_msa.qkv.weight, backbone.stages.2.blocks.8.attn.w_msa.qkv.bias, backbone.stages.2.blocks.8.attn.w_msa.proj.weight, backbone.stages.2.blocks.8.attn.w_msa.proj.bias, backbone.stages.2.blocks.8.norm2.weight, backbone.stages.2.blocks.8.norm2.bias, backbone.stages.2.blocks.8.ffn.layers.0.0.weight, backbone.stages.2.blocks.8.ffn.layers.0.0.bias, backbone.stages.2.blocks.8.ffn.layers.1.weight, backbone.stages.2.blocks.8.ffn.layers.1.bias, backbone.stages.2.blocks.9.norm1.weight, backbone.stages.2.blocks.9.norm1.bias, backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.9.attn.w_msa.relative_position_index, backbone.stages.2.blocks.9.attn.w_msa.qkv.weight, backbone.stages.2.blocks.9.attn.w_msa.qkv.bias, backbone.stages.2.blocks.9.attn.w_msa.proj.weight, backbone.stages.2.blocks.9.attn.w_msa.proj.bias, backbone.stages.2.blocks.9.norm2.weight, backbone.stages.2.blocks.9.norm2.bias, backbone.stages.2.blocks.9.ffn.layers.0.0.weight, backbone.stages.2.blocks.9.ffn.layers.0.0.bias, backbone.stages.2.blocks.9.ffn.layers.1.weight, backbone.stages.2.blocks.9.ffn.layers.1.bias, backbone.stages.2.blocks.10.norm1.weight, backbone.stages.2.blocks.10.norm1.bias, backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.10.attn.w_msa.relative_position_index, backbone.stages.2.blocks.10.attn.w_msa.qkv.weight, backbone.stages.2.blocks.10.attn.w_msa.qkv.bias, backbone.stages.2.blocks.10.attn.w_msa.proj.weight, backbone.stages.2.blocks.10.attn.w_msa.proj.bias, backbone.stages.2.blocks.10.norm2.weight, backbone.stages.2.blocks.10.norm2.bias, backbone.stages.2.blocks.10.ffn.layers.0.0.weight, backbone.stages.2.blocks.10.ffn.layers.0.0.bias, backbone.stages.2.blocks.10.ffn.layers.1.weight, backbone.stages.2.blocks.10.ffn.layers.1.bias, backbone.stages.2.blocks.11.norm1.weight, backbone.stages.2.blocks.11.norm1.bias, backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.11.attn.w_msa.relative_position_index, backbone.stages.2.blocks.11.attn.w_msa.qkv.weight, backbone.stages.2.blocks.11.attn.w_msa.qkv.bias, backbone.stages.2.blocks.11.attn.w_msa.proj.weight, backbone.stages.2.blocks.11.attn.w_msa.proj.bias, backbone.stages.2.blocks.11.norm2.weight, backbone.stages.2.blocks.11.norm2.bias, backbone.stages.2.blocks.11.ffn.layers.0.0.weight, backbone.stages.2.blocks.11.ffn.layers.0.0.bias, backbone.stages.2.blocks.11.ffn.layers.1.weight, backbone.stages.2.blocks.11.ffn.layers.1.bias, backbone.stages.2.blocks.12.norm1.weight, backbone.stages.2.blocks.12.norm1.bias, backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.12.attn.w_msa.relative_position_index, backbone.stages.2.blocks.12.attn.w_msa.qkv.weight, backbone.stages.2.blocks.12.attn.w_msa.qkv.bias, backbone.stages.2.blocks.12.attn.w_msa.proj.weight, backbone.stages.2.blocks.12.attn.w_msa.proj.bias, backbone.stages.2.blocks.12.norm2.weight, backbone.stages.2.blocks.12.norm2.bias, backbone.stages.2.blocks.12.ffn.layers.0.0.weight, backbone.stages.2.blocks.12.ffn.layers.0.0.bias, backbone.stages.2.blocks.12.ffn.layers.1.weight, backbone.stages.2.blocks.12.ffn.layers.1.bias, backbone.stages.2.blocks.13.norm1.weight, backbone.stages.2.blocks.13.norm1.bias, backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.13.attn.w_msa.relative_position_index, backbone.stages.2.blocks.13.attn.w_msa.qkv.weight, backbone.stages.2.blocks.13.attn.w_msa.qkv.bias, backbone.stages.2.blocks.13.attn.w_msa.proj.weight, backbone.stages.2.blocks.13.attn.w_msa.proj.bias, backbone.stages.2.blocks.13.norm2.weight, backbone.stages.2.blocks.13.norm2.bias, backbone.stages.2.blocks.13.ffn.layers.0.0.weight, backbone.stages.2.blocks.13.ffn.layers.0.0.bias, backbone.stages.2.blocks.13.ffn.layers.1.weight, backbone.stages.2.blocks.13.ffn.layers.1.bias, backbone.stages.2.blocks.14.norm1.weight, backbone.stages.2.blocks.14.norm1.bias, backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.14.attn.w_msa.relative_position_index, backbone.stages.2.blocks.14.attn.w_msa.qkv.weight, backbone.stages.2.blocks.14.attn.w_msa.qkv.bias, backbone.stages.2.blocks.14.attn.w_msa.proj.weight, backbone.stages.2.blocks.14.attn.w_msa.proj.bias, backbone.stages.2.blocks.14.norm2.weight, backbone.stages.2.blocks.14.norm2.bias, backbone.stages.2.blocks.14.ffn.layers.0.0.weight, backbone.stages.2.blocks.14.ffn.layers.0.0.bias, backbone.stages.2.blocks.14.ffn.layers.1.weight, backbone.stages.2.blocks.14.ffn.layers.1.bias, backbone.stages.2.blocks.15.norm1.weight, backbone.stages.2.blocks.15.norm1.bias, backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.15.attn.w_msa.relative_position_index, backbone.stages.2.blocks.15.attn.w_msa.qkv.weight, backbone.stages.2.blocks.15.attn.w_msa.qkv.bias, backbone.stages.2.blocks.15.attn.w_msa.proj.weight, backbone.stages.2.blocks.15.attn.w_msa.proj.bias, backbone.stages.2.blocks.15.norm2.weight, backbone.stages.2.blocks.15.norm2.bias, backbone.stages.2.blocks.15.ffn.layers.0.0.weight, backbone.stages.2.blocks.15.ffn.layers.0.0.bias, backbone.stages.2.blocks.15.ffn.layers.1.weight, backbone.stages.2.blocks.15.ffn.layers.1.bias, backbone.stages.2.blocks.16.norm1.weight, backbone.stages.2.blocks.16.norm1.bias, backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.16.attn.w_msa.relative_position_index, backbone.stages.2.blocks.16.attn.w_msa.qkv.weight, backbone.stages.2.blocks.16.attn.w_msa.qkv.bias, backbone.stages.2.blocks.16.attn.w_msa.proj.weight, backbone.stages.2.blocks.16.attn.w_msa.proj.bias, backbone.stages.2.blocks.16.norm2.weight, backbone.stages.2.blocks.16.norm2.bias, backbone.stages.2.blocks.16.ffn.layers.0.0.weight, backbone.stages.2.blocks.16.ffn.layers.0.0.bias, backbone.stages.2.blocks.16.ffn.layers.1.weight, backbone.stages.2.blocks.16.ffn.layers.1.bias, backbone.stages.2.blocks.17.norm1.weight, backbone.stages.2.blocks.17.norm1.bias, backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.17.attn.w_msa.relative_position_index, backbone.stages.2.blocks.17.attn.w_msa.qkv.weight, backbone.stages.2.blocks.17.attn.w_msa.qkv.bias, backbone.stages.2.blocks.17.attn.w_msa.proj.weight, backbone.stages.2.blocks.17.attn.w_msa.proj.bias, backbone.stages.2.blocks.17.norm2.weight, backbone.stages.2.blocks.17.norm2.bias, backbone.stages.2.blocks.17.ffn.layers.0.0.weight, backbone.stages.2.blocks.17.ffn.layers.0.0.bias, backbone.stages.2.blocks.17.ffn.layers.1.weight, backbone.stages.2.blocks.17.ffn.layers.1.bias, backbone.stages.2.downsample.norm.weight, backbone.stages.2.downsample.norm.bias, backbone.stages.2.downsample.reduction.weight, backbone.stages.3.blocks.0.norm1.weight, backbone.stages.3.blocks.0.norm1.bias, backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table, backbone.stages.3.blocks.0.attn.w_msa.relative_position_index, backbone.stages.3.blocks.0.attn.w_msa.qkv.weight, backbone.stages.3.blocks.0.attn.w_msa.qkv.bias, backbone.stages.3.blocks.0.attn.w_msa.proj.weight, backbone.stages.3.blocks.0.attn.w_msa.proj.bias, backbone.stages.3.blocks.0.norm2.weight, backbone.stages.3.blocks.0.norm2.bias, backbone.stages.3.blocks.0.ffn.layers.0.0.weight, backbone.stages.3.blocks.0.ffn.layers.0.0.bias, backbone.stages.3.blocks.0.ffn.layers.1.weight, backbone.stages.3.blocks.0.ffn.layers.1.bias, backbone.stages.3.blocks.1.norm1.weight, backbone.stages.3.blocks.1.norm1.bias, backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table, backbone.stages.3.blocks.1.attn.w_msa.relative_position_index, backbone.stages.3.blocks.1.attn.w_msa.qkv.weight, backbone.stages.3.blocks.1.attn.w_msa.qkv.bias, backbone.stages.3.blocks.1.attn.w_msa.proj.weight, backbone.stages.3.blocks.1.attn.w_msa.proj.bias, backbone.stages.3.blocks.1.norm2.weight, backbone.stages.3.blocks.1.norm2.bias, backbone.stages.3.blocks.1.ffn.layers.0.0.weight, backbone.stages.3.blocks.1.ffn.layers.0.0.bias, backbone.stages.3.blocks.1.ffn.layers.1.weight, backbone.stages.3.blocks.1.ffn.layers.1.bias, backbone.norm0.weight, backbone.norm0.bias, backbone.norm1.weight, backbone.norm1.bias, backbone.norm2.weight, backbone.norm2.bias, backbone.norm3.weight, backbone.norm3.bias, decode_head.conv_seg.weight, decode_head.conv_seg.bias, decode_head.psp_modules.0.1.conv.weight, decode_head.psp_modules.0.1.bn.weight, decode_head.psp_modules.0.1.bn.bias, decode_head.psp_modules.0.1.bn.running_mean, decode_head.psp_modules.0.1.bn.running_var, decode_head.psp_modules.1.1.conv.weight, decode_head.psp_modules.1.1.bn.weight, decode_head.psp_modules.1.1.bn.bias, decode_head.psp_modules.1.1.bn.running_mean, decode_head.psp_modules.1.1.bn.running_var, decode_head.psp_modules.2.1.conv.weight, decode_head.psp_modules.2.1.bn.weight, decode_head.psp_modules.2.1.bn.bias, decode_head.psp_modules.2.1.bn.running_mean, decode_head.psp_modules.2.1.bn.running_var, decode_head.psp_modules.3.1.conv.weight, decode_head.psp_modules.3.1.bn.weight, decode_head.psp_modules.3.1.bn.bias, decode_head.psp_modules.3.1.bn.running_mean, decode_head.psp_modules.3.1.bn.running_var, decode_head.bottleneck.conv.weight, decode_head.bottleneck.bn.weight, decode_head.bottleneck.bn.bias, decode_head.bottleneck.bn.running_mean, decode_head.bottleneck.bn.running_var, decode_head.lateral_convs.0.conv.weight, decode_head.lateral_convs.0.bn.weight, decode_head.lateral_convs.0.bn.bias, decode_head.lateral_convs.0.bn.running_mean, decode_head.lateral_convs.0.bn.running_var, decode_head.lateral_convs.1.conv.weight, decode_head.lateral_convs.1.bn.weight, decode_head.lateral_convs.1.bn.bias, decode_head.lateral_convs.1.bn.running_mean, decode_head.lateral_convs.1.bn.running_var, decode_head.lateral_convs.2.conv.weight, decode_head.lateral_convs.2.bn.weight, decode_head.lateral_convs.2.bn.bias, decode_head.lateral_convs.2.bn.running_mean, decode_head.lateral_convs.2.bn.running_var, decode_head.fpn_convs.0.conv.weight, decode_head.fpn_convs.0.bn.weight, decode_head.fpn_convs.0.bn.bias, decode_head.fpn_convs.0.bn.running_mean, decode_head.fpn_convs.0.bn.running_var, decode_head.fpn_convs.1.conv.weight, decode_head.fpn_convs.1.bn.weight, decode_head.fpn_convs.1.bn.bias, decode_head.fpn_convs.1.bn.running_mean, decode_head.fpn_convs.1.bn.running_var, decode_head.fpn_convs.2.conv.weight, decode_head.fpn_convs.2.bn.weight, decode_head.fpn_convs.2.bn.bias, decode_head.fpn_convs.2.bn.running_mean, decode_head.fpn_convs.2.bn.running_var, decode_head.fpn_bottleneck.conv.weight, decode_head.fpn_bottleneck.bn.weight, decode_head.fpn_bottleneck.bn.bias, decode_head.fpn_bottleneck.bn.running_mean, decode_head.fpn_bottleneck.bn.running_var, auxiliary_head.conv_seg.weight, auxiliary_head.conv_seg.bias, auxiliary_head.convs.0.conv.weight, auxiliary_head.convs.0.bn.weight, auxiliary_head.convs.0.bn.bias, auxiliary_head.convs.0.bn.running_mean, auxiliary_head.convs.0.bn.running_var\n",
      "\n",
      "2022-10-06 09:56:48,528 - mmseg - INFO - Start running, host: root@c582f55fa2bc, work_dir: /data/result/36-3/test1006\n",
      "2022-10-06 09:56:48,537 - mmseg - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) PolyLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) PolyLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) PolyLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_run:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2022-10-06 09:56:48,548 - mmseg - INFO - workflow: [('train', 1)], max: 160000 iters\n",
      "2022-10-06 09:56:48,549 - mmseg - INFO - Checkpoints will be saved to /data/result/36-3/test1006 by HardDiskBackend.\n",
      "2022-10-06 10:00:22,588 - mmseg - INFO - Iter [200/160000]\tlr: 7.950e-06, eta: 1 day, 23:25:02, time: 1.068, data_time: 0.028, memory: 23780, decode.loss_ce: 163.9188, decode.acc_seg: 2.0469, aux.loss_ce: 99.6393, aux.acc_seg: 1.2078, loss: 263.5581\n",
      "2022-10-06 10:03:51,897 - mmseg - INFO - Iter [400/160000]\tlr: 1.592e-05, eta: 1 day, 22:52:37, time: 1.047, data_time: 0.015, memory: 23780, decode.loss_ce: 46.5660, decode.acc_seg: 2.6212, aux.loss_ce: 25.7906, aux.acc_seg: 1.0566, loss: 72.3566\n",
      "2022-10-06 10:07:24,981 - mmseg - INFO - Iter [600/160000]\tlr: 2.387e-05, eta: 1 day, 22:56:10, time: 1.065, data_time: 0.035, memory: 23780, decode.loss_ce: 38.8121, decode.acc_seg: 5.2358, aux.loss_ce: 21.0667, aux.acc_seg: 1.1924, loss: 59.8789\n",
      "2022-10-06 10:10:54,168 - mmseg - INFO - Iter [800/160000]\tlr: 3.180e-05, eta: 1 day, 22:43:18, time: 1.046, data_time: 0.016, memory: 23780, decode.loss_ce: 30.7186, decode.acc_seg: 13.4866, aux.loss_ce: 18.4051, aux.acc_seg: 1.2127, loss: 49.1237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 912/912, 3.8 task/s, elapsed: 238s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-06 10:14:52,027 - mmseg - INFO - per class results:\n",
      "2022-10-06 10:14:52,035 - mmseg - INFO - \n",
      "+-------------------+-------+-------+\n",
      "|       Class       |  IoU  |  Acc  |\n",
      "+-------------------+-------+-------+\n",
      "|     background    | 44.28 | 44.31 |\n",
      "|      vehicle      |  0.0  |  nan  |\n",
      "|        bus        |  0.0  |  0.0  |\n",
      "|       truck       |  0.0  |  0.0  |\n",
      "|     policeCar     |  2.01 | 56.54 |\n",
      "|     ambulance     |  0.0  |  0.0  |\n",
      "|     schoolBus     |  0.0  |  0.0  |\n",
      "|      otherCar     |  0.0  |  0.0  |\n",
      "|     motorcycle    |  0.44 | 50.87 |\n",
      "|      bicycle      |  0.0  |  0.0  |\n",
      "|     twoWheeler    |  0.0  |  0.0  |\n",
      "|     pedestrian    |  0.0  |  nan  |\n",
      "|       rider       |  0.0  |  0.0  |\n",
      "|     freespace     |  0.0  |  0.0  |\n",
      "|        curb       |  0.0  |  0.0  |\n",
      "|      sidewalk     |  8.63 | 13.41 |\n",
      "|     crossWalk     |  0.0  |  0.0  |\n",
      "|     safetyZone    |  0.0  |  0.0  |\n",
      "|     speedBump     |  3.63 | 99.56 |\n",
      "|      roadMark     |  0.0  |  nan  |\n",
      "|     whiteLane     |  0.0  |  0.0  |\n",
      "|     yellowLane    |  0.0  |  0.0  |\n",
      "|      blueLane     |  0.0  |  0.0  |\n",
      "|      redLane      | 23.26 | 34.27 |\n",
      "|      stopLane     |  0.0  |  nan  |\n",
      "| constructionGuide |  0.0  |  0.0  |\n",
      "|    trafficDrum    |  0.0  |  0.0  |\n",
      "|     rubberCone    |  0.0  |  nan  |\n",
      "|    trafficSign    |  0.0  |  0.0  |\n",
      "|    trafficLight   |  0.96 |  2.03 |\n",
      "|  warningTriangle  |  0.0  |  0.0  |\n",
      "|       fence       |  nan  |  nan  |\n",
      "+-------------------+-------+-------+\n",
      "2022-10-06 10:14:52,045 - mmseg - INFO - Summary:\n",
      "2022-10-06 10:14:52,045 - mmseg - INFO - \n",
      "+-------+------+-------+\n",
      "|  aAcc | mIoU |  mAcc |\n",
      "+-------+------+-------+\n",
      "| 36.56 | 2.68 | 11.58 |\n",
      "+-------+------+-------+\n",
      "2022-10-06 10:14:52,046 - mmseg - INFO - Iter(val) [912]\taAcc: 0.3656, mIoU: 0.0268, mAcc: 0.1158, IoU.background: 0.4428, IoU.vehicle: 0.0000, IoU.bus: 0.0000, IoU.truck: 0.0000, IoU.policeCar: 0.0201, IoU.ambulance: 0.0000, IoU.schoolBus: 0.0000, IoU.otherCar: 0.0000, IoU.motorcycle: 0.0044, IoU.bicycle: 0.0000, IoU.twoWheeler: 0.0000, IoU.pedestrian: 0.0000, IoU.rider: 0.0000, IoU.freespace: 0.0000, IoU.curb: 0.0000, IoU.sidewalk: 0.0863, IoU.crossWalk: 0.0000, IoU.safetyZone: 0.0000, IoU.speedBump: 0.0363, IoU.roadMark: 0.0000, IoU.whiteLane: 0.0000, IoU.yellowLane: 0.0000, IoU.blueLane: 0.0000, IoU.redLane: 0.2326, IoU.stopLane: 0.0000, IoU.constructionGuide: 0.0000, IoU.trafficDrum: 0.0000, IoU.rubberCone: 0.0000, IoU.trafficSign: 0.0000, IoU.trafficLight: 0.0096, IoU.warningTriangle: 0.0000, IoU.fence: nan, Acc.background: 0.4431, Acc.vehicle: nan, Acc.bus: 0.0000, Acc.truck: 0.0000, Acc.policeCar: 0.5654, Acc.ambulance: 0.0000, Acc.schoolBus: 0.0000, Acc.otherCar: 0.0000, Acc.motorcycle: 0.5087, Acc.bicycle: 0.0000, Acc.twoWheeler: 0.0000, Acc.pedestrian: nan, Acc.rider: 0.0000, Acc.freespace: 0.0000, Acc.curb: 0.0000, Acc.sidewalk: 0.1341, Acc.crossWalk: 0.0000, Acc.safetyZone: 0.0000, Acc.speedBump: 0.9956, Acc.roadMark: nan, Acc.whiteLane: 0.0000, Acc.yellowLane: 0.0000, Acc.blueLane: 0.0000, Acc.redLane: 0.3427, Acc.stopLane: nan, Acc.constructionGuide: 0.0000, Acc.trafficDrum: 0.0000, Acc.rubberCone: nan, Acc.trafficSign: 0.0000, Acc.trafficLight: 0.0203, Acc.warningTriangle: 0.0000, Acc.fence: nan\n",
      "2022-10-06 10:18:24,080 - mmseg - INFO - Iter [1000/160000]\tlr: 3.971e-05, eta: 2 days, 9:12:05, time: 2.250, data_time: 1.225, memory: 23780, decode.loss_ce: 26.2446, decode.acc_seg: 37.2242, aux.loss_ce: 17.3956, aux.acc_seg: 1.2990, loss: 43.6402\n",
      "2022-10-06 10:21:53,218 - mmseg - INFO - Iter [1200/160000]\tlr: 4.760e-05, eta: 2 days, 7:17:43, time: 1.046, data_time: 0.016, memory: 23780, decode.loss_ce: 23.1163, decode.acc_seg: 39.1438, aux.loss_ce: 16.2630, aux.acc_seg: 1.4127, loss: 39.3793\n",
      "2022-10-06 10:25:26,140 - mmseg - INFO - Iter [1400/160000]\tlr: 5.547e-05, eta: 2 days, 6:02:10, time: 1.065, data_time: 0.036, memory: 23780, decode.loss_ce: 22.7474, decode.acc_seg: 40.4822, aux.loss_ce: 15.7117, aux.acc_seg: 2.1301, loss: 38.4591\n",
      "2022-10-06 10:28:55,244 - mmseg - INFO - Iter [1600/160000]\tlr: 5.940e-05, eta: 2 days, 4:58:21, time: 1.046, data_time: 0.016, memory: 23780, decode.loss_ce: 19.5627, decode.acc_seg: 41.4504, aux.loss_ce: 13.8096, aux.acc_seg: 11.1207, loss: 33.3723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 912/912, 4.0 task/s, elapsed: 230s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-06 10:32:45,016 - mmseg - INFO - per class results:\n",
      "2022-10-06 10:32:45,022 - mmseg - INFO - \n",
      "+-------------------+-------+-------+\n",
      "|       Class       |  IoU  |  Acc  |\n",
      "+-------------------+-------+-------+\n",
      "|     background    | 57.83 | 57.94 |\n",
      "|      vehicle      |  nan  |  nan  |\n",
      "|        bus        |  0.0  |  0.0  |\n",
      "|       truck       |  0.0  |  0.0  |\n",
      "|     policeCar     |  2.67 | 74.56 |\n",
      "|     ambulance     |  6.37 | 85.12 |\n",
      "|     schoolBus     |  0.0  |  0.0  |\n",
      "|      otherCar     |  0.0  |  0.0  |\n",
      "|     motorcycle    |  0.89 | 77.35 |\n",
      "|      bicycle      |  0.0  |  0.0  |\n",
      "|     twoWheeler    |  0.0  |  0.0  |\n",
      "|     pedestrian    |  0.0  |  nan  |\n",
      "|       rider       |  0.0  |  0.0  |\n",
      "|     freespace     |  0.0  |  0.0  |\n",
      "|        curb       |  0.0  |  0.0  |\n",
      "|      sidewalk     | 22.63 | 49.96 |\n",
      "|     crossWalk     |  0.0  |  0.0  |\n",
      "|     safetyZone    |  0.0  |  0.0  |\n",
      "|     speedBump     |  4.43 | 99.76 |\n",
      "|      roadMark     |  nan  |  nan  |\n",
      "|     whiteLane     |  0.0  |  0.0  |\n",
      "|     yellowLane    |  0.0  |  0.0  |\n",
      "|      blueLane     |  0.0  |  0.0  |\n",
      "|      redLane      | 31.12 | 41.98 |\n",
      "|      stopLane     |  nan  |  nan  |\n",
      "| constructionGuide |  0.0  |  0.0  |\n",
      "|    trafficDrum    |  0.0  |  0.0  |\n",
      "|     rubberCone    |  nan  |  nan  |\n",
      "|    trafficSign    |  0.0  |  0.0  |\n",
      "|    trafficLight   |  0.12 |  0.12 |\n",
      "|  warningTriangle  |  0.01 |  57.0 |\n",
      "|       fence       |  nan  |  nan  |\n",
      "+-------------------+-------+-------+\n",
      "2022-10-06 10:32:45,032 - mmseg - INFO - Summary:\n",
      "2022-10-06 10:32:45,033 - mmseg - INFO - \n",
      "+-------+------+-------+\n",
      "|  aAcc | mIoU |  mAcc |\n",
      "+-------+------+-------+\n",
      "| 47.91 | 4.67 | 20.92 |\n",
      "+-------+------+-------+\n",
      "2022-10-06 10:32:45,033 - mmseg - INFO - Iter(val) [912]\taAcc: 0.4791, mIoU: 0.0467, mAcc: 0.2092, IoU.background: 0.5783, IoU.vehicle: nan, IoU.bus: 0.0000, IoU.truck: 0.0000, IoU.policeCar: 0.0267, IoU.ambulance: 0.0637, IoU.schoolBus: 0.0000, IoU.otherCar: 0.0000, IoU.motorcycle: 0.0089, IoU.bicycle: 0.0000, IoU.twoWheeler: 0.0000, IoU.pedestrian: 0.0000, IoU.rider: 0.0000, IoU.freespace: 0.0000, IoU.curb: 0.0000, IoU.sidewalk: 0.2263, IoU.crossWalk: 0.0000, IoU.safetyZone: 0.0000, IoU.speedBump: 0.0443, IoU.roadMark: nan, IoU.whiteLane: 0.0000, IoU.yellowLane: 0.0000, IoU.blueLane: 0.0000, IoU.redLane: 0.3112, IoU.stopLane: nan, IoU.constructionGuide: 0.0000, IoU.trafficDrum: 0.0000, IoU.rubberCone: nan, IoU.trafficSign: 0.0000, IoU.trafficLight: 0.0012, IoU.warningTriangle: 0.0001, IoU.fence: nan, Acc.background: 0.5794, Acc.vehicle: nan, Acc.bus: 0.0000, Acc.truck: 0.0000, Acc.policeCar: 0.7456, Acc.ambulance: 0.8512, Acc.schoolBus: 0.0000, Acc.otherCar: 0.0000, Acc.motorcycle: 0.7735, Acc.bicycle: 0.0000, Acc.twoWheeler: 0.0000, Acc.pedestrian: nan, Acc.rider: 0.0000, Acc.freespace: 0.0000, Acc.curb: 0.0000, Acc.sidewalk: 0.4996, Acc.crossWalk: 0.0000, Acc.safetyZone: 0.0000, Acc.speedBump: 0.9976, Acc.roadMark: nan, Acc.whiteLane: 0.0000, Acc.yellowLane: 0.0000, Acc.blueLane: 0.0000, Acc.redLane: 0.4198, Acc.stopLane: nan, Acc.constructionGuide: 0.0000, Acc.trafficDrum: 0.0000, Acc.rubberCone: nan, Acc.trafficSign: 0.0000, Acc.trafficLight: 0.0012, Acc.warningTriangle: 0.5700, Acc.fence: nan\n",
      "2022-10-06 10:36:12,539 - mmseg - INFO - Iter [1800/160000]\tlr: 5.933e-05, eta: 2 days, 9:42:11, time: 2.186, data_time: 1.163, memory: 23780, decode.loss_ce: 18.9823, decode.acc_seg: 41.8988, aux.loss_ce: 13.5036, aux.acc_seg: 16.5672, loss: 32.4859\n",
      "2022-10-06 10:39:44,515 - mmseg - INFO - Iter [2000/160000]\tlr: 5.925e-05, eta: 2 days, 8:31:08, time: 1.060, data_time: 0.034, memory: 23780, decode.loss_ce: 18.0997, decode.acc_seg: 42.9453, aux.loss_ce: 12.8525, aux.acc_seg: 21.5969, loss: 30.9521\n",
      "2022-10-06 10:43:13,114 - mmseg - INFO - Iter [2200/160000]\tlr: 5.918e-05, eta: 2 days, 7:28:19, time: 1.043, data_time: 0.015, memory: 23780, decode.loss_ce: 18.4813, decode.acc_seg: 44.2164, aux.loss_ce: 12.7705, aux.acc_seg: 23.1211, loss: 31.2518\n"
     ]
    }
   ],
   "source": [
    "from mmseg.datasets import build_dataset, build_dataloader\n",
    "from mmseg.models import build_segmentor\n",
    "from mmseg.apis import train_segmentor\n",
    "\n",
    "# Clear Cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# print(datasets[0][0]['img'].__dict__)\n",
    "#print(datasets[0][0])\n",
    "\n",
    "# Build the detector\n",
    "model = build_segmentor(cfg.model)\n",
    "# Add an attribute for visualization convenience\n",
    "model.CLASSES = datasets[0].CLASSES\n",
    "print(len(model.CLASSES))\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_segmentor(model, datasets, cfg, distributed=False, validate=True, \n",
    "                meta=dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = [\n",
    "                [0,0,0],[128, 0, 0], [0, 128, 0], [0, 0, 128], [128, 128, 0],  [128, 0, 128], [0, 128, 128], [128, 128, 128], \n",
    "                [64, 0, 0], [0, 64, 0], [0, 0, 64], [64, 64, 0],  [64, 0, 64], [0, 64, 64], [64, 64, 64], \n",
    "                [192, 0, 0], [0, 192, 0], [0, 0, 192], [192, 192, 0],  [192, 0, 192], [0, 192, 192], [192, 192, 192], \n",
    "                [64, 128, 0], [192, 128, 0], [64, 0, 128], [192, 0, 128], [64, 128, 128], \n",
    "                [192, 128, 128], [128, 64, 0], [0, 192, 128], [128, 192, 0], [0, 64, 128]\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ade_custom\n",
    "model = torch.load(cfg.work_dir+'latest.pth')\n",
    "model['meta']['PALETTE'] = palette\n",
    "torch.save(model, cfg.work_dir+'latest_fix.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Inference with trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = '/home/mmsegmentation/custom_config_test.py'\n",
    "checkpoint_file = '/data/result/36-3/test0928_2/latest_fix.pth'\n",
    "\n",
    "model = init_segmentor(config_file, checkpoint_file, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmseg.apis import inference_segmentor, init_segmentor, show_result_pyplot\n",
    "from mmseg.core.evaluation import get_palette\n",
    "\n",
    "img = '/data/36-3/img_test/16_142441_220613_150.jpg'\n",
    "\n",
    "result = inference_segmentor(model, img)\n",
    "\n",
    "show_result_pyplot(model, img, result, palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('openmmlab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c745df823b0b8fbc96bd327094d24a497ca88aeabc85de830d0531f0a8d26eb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
